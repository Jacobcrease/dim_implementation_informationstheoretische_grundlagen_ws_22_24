{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Workaround for Pylance\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pathes = [\n",
    "    \"./models/standard_classifier\",\n",
    "    \"./models/global_dim_classifier\",\n",
    "    \"./models/local_dim_classifier\",\n",
    "    \"./models/mixed_dim_classifier\",\n",
    "    \"./models/complete_dim_classifier\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_classifier = keras.models.load_model(model_pathes[0])\n",
    "global_classifier = keras.models.load_model(model_pathes[1])\n",
    "local_classifier = keras.models.load_model(model_pathes[2])\n",
    "mixed_classifier = keras.models.load_model(model_pathes[3])\n",
    "completed_classifier = keras.models.load_model(model_pathes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(_, _), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "global_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "local_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "mixed_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "completed_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=10000.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_logits = standard_classifier(x_test, training=False)\n",
    "standard_acc_metric.update_state(y_test, standard_logits)\n",
    "\n",
    "global_logits = global_classifier(x_test, training=False)\n",
    "global_acc_metric.update_state(y_test, global_logits)\n",
    "\n",
    "local_logits = local_classifier(x_test, training=False)\n",
    "local_acc_metric.update_state(y_test, local_logits)\n",
    "\n",
    "mixed_logits = mixed_classifier(x_test, training=False)\n",
    "mixed_acc_metric.update_state(y_test, mixed_logits)\n",
    "\n",
    "completed_logits = completed_classifier(x_test, training=False)\n",
    "completed_acc_metric.update_state(y_test, completed_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [\n",
    "    standard_acc_metric.result().numpy(),\n",
    "    global_acc_metric.result().numpy(),\n",
    "    local_acc_metric.result().numpy(),\n",
    "    mixed_acc_metric.result().numpy(),\n",
    "    completed_acc_metric.result().numpy(),\n",
    "]\n",
    "\n",
    "labels = [\"standard\", \"global\", \"local\", \"mixed\", \"complete\"]\n",
    "wheights = [\n",
    "    \"\",\n",
    "    \"g=1.0 l=0.0 p=1.0\",\n",
    "    \"g=0.0 l=1.0 p=0.1\",\n",
    "    \"g=0.6 l=0.4 p=0.0\",\n",
    "    \"g=0.6 l=0.4 p=0.2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Classifier    | Encoder Wheights  | accuracity     \n",
      "-------------------|-------------------|---------------\n",
      "standard           |                   | 0.8999000191688538\n",
      "global             | g=1.0 l=0.0 p=1.0 | 0.9215999841690063\n",
      "local              | g=0.0 l=1.0 p=0.1 | 0.9631999731063843\n",
      "mixed              | g=0.6 l=0.4 p=0.0 | 0.9197999835014343\n",
      "complete           | g=0.6 l=0.4 p=0.2 | 0.9472000002861023\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"{:<18} | {:<17} | {:<15}\".format(\"Used Classifier\", \"Encoder Wheights\", \"Accuracy\")\n",
    ")\n",
    "print(\"-\" * 19 + \"|\" + \"-\" * 19 + \"|\" + \"-\" * 15)\n",
    "for index in range(5):\n",
    "    print(\n",
    "        \"{:<18} | {:<17} | {:<15}\".format(\n",
    "            labels[index], wheights[index], accuracy[index]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Data\n",
    "In the results, it can be seen that all the models trained with the specialized encoder have higher accuracy. However, the result also shows that the global task seems to be less good than the local one.\n",
    "\n",
    "The authors of the original paper assume that the local dim task helps to ignore unimportant features.\n",
    "Since the images in the mnist dataset consist of a lot of \"background\", this could be a possible answer as to why this is the case.\n",
    "\n",
    "It also shows that accuracy decreases as soon as we turn off prior matching, which is supposed to counteract possible mode collapse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In their paper, the authors also tried standard data sets to test their approach.\n",
    "Which mostly resulted in really amazing results in which simple supervised learning was almost always surpassed.\n",
    "\n",
    "That made me sceptical, but my results tend to back it.\n",
    "\n",
    "But this was a very simple task.\n",
    "The data within the paper, also show a slight trend, that this could discrease with more complex tasks/larger images.\n",
    "\n",
    "A good next step would be to expand the flexibility of my implementation and try more and better measurements with more complex datasets.\n",
    "In the paper they also introduced much more measurement tools, which would also be interesting to try out, as they would help to understand how well different parameters affects help with maximizing MIin the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_acc_metric.reset_states()\n",
    "global_acc_metric.reset_states()\n",
    "local_acc_metric.reset_states()\n",
    "mixed_acc_metric.reset_states()\n",
    "completed_acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
